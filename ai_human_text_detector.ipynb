{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7802,
     "status": "ok",
     "timestamp": 1770450729617,
     "user": {
      "displayName": "4102_Ravi Chaudhary",
      "userId": "03292946989340673993"
     },
     "user_tz": -330
    },
    "id": "bSeP4ZqG6jq9",
    "outputId": "8ab97ab1-7df3-4310-80ee-67361d6b5a5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (1.3.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.24.0->datasets) (8.3.1)\n",
      "                                               title  \\\n",
      "0  Epigenetic inheritance of circadian period in ...   \n",
      "1  Pediatric Airway Stent Designed to Facilitate ...   \n",
      "2  Infection prevention and control in paediatric...   \n",
      "3  Correlation between thyroid function, testoste...   \n",
      "4  Natural intertypic and intratypic recombinants...   \n",
      "\n",
      "                                            abstract  label  \n",
      "0  \\n\\nThis study focuses on the epigenetic inher...      1  \n",
      "1  Objective: The goal was to develop a pediatric...      0  \n",
      "2  Transmission of infection in the paediatric of...      0  \n",
      "3  STUDY DESIGN: Prospective case series. OBJECTI...      0  \n",
      "4  \\n\\nThis study aims to analyze the recombinant...      1  \n",
      "label\n",
      "1    11465\n",
      "0    11465\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"Ateeqq/AI-and-Human-Generated-Text\")\n",
    "\n",
    "\n",
    "df = dataset[\"train\"].to_pandas()\n",
    "\n",
    "print(df.head())\n",
    "print(df[\"label\"].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13828,
     "status": "ok",
     "timestamp": 1770450743449,
     "user": {
      "displayName": "4102_Ravi Chaudhary",
      "userId": "03292946989340673993"
     },
     "user_tz": -330
    },
    "id": "IxHe1LRx8T2z",
    "outputId": "9a3a2b07-a475-409e-cb74-99a1bec9b6b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning: (22930, 3)\n",
      "After cleaning: (21612, 2)\n",
      "label\n",
      "0    10821\n",
      "1    10791\n",
      "Name: count, dtype: int64\n",
      "âœ… Saved: /content/drive/MyDrive/ai_human_detection/final_clean_ai_human_text.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Before cleaning:\", df.shape)\n",
    "\n",
    "\n",
    "df[\"text\"] = df[\"title\"].fillna(\"\") + \" \" + df[\"abstract\"].fillna(\"\")\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "\n",
    "df[\"word_count\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "df = df[(df[\"word_count\"] >= 80) & (df[\"word_count\"] <= 400)]\n",
    "\n",
    "\n",
    "df = df[[\"text\", \"label\"]].reset_index(drop=True)\n",
    "\n",
    "print(\"After cleaning:\", df.shape)\n",
    "print(df[\"label\"].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1770450743523,
     "user": {
      "displayName": "4102_Ravi Chaudhary",
      "userId": "03292946989340673993"
     },
     "user_tz": -330
    },
    "id": "2lMy17-b9o6K",
    "outputId": "ed83ef63-11d4-4563-ebfc-1f737b110189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All NLTK resources installed correctly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# FINAL FIX: NLTK POS TAGGER\n",
    "\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 285419,
     "status": "ok",
     "timestamp": 1770451028945,
     "user": {
      "displayName": "4102_Ravi Chaudhary",
      "userId": "03292946989340673993"
     },
     "user_tz": -330
    },
    "id": "EeVYW01f9ZJD",
    "outputId": "b82fc02e-a064-4d6e-9b2a-767804f8a992"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: (21612, 2)\n",
      " Extracting linguistic features (this WILL take time)...\n",
      "âœ… Linguistic features extracted\n",
      "   word_count  sentence_count  avg_sentence_length  lexical_diversity  \\\n",
      "0         203               1                203.0           0.783251   \n",
      "1         250               1                250.0           0.536000   \n",
      "2         103               1                103.0           0.669903   \n",
      "3         264               1                264.0           0.465909   \n",
      "4         132               1                132.0           0.757576   \n",
      "\n",
      "   noun_ratio  verb_ratio  adj_ratio  stopword_ratio  readability  label  \n",
      "0    0.320197    0.162562   0.182266        0.320197  -190.497685      1  \n",
      "1    0.336000    0.180000   0.112000        0.352000  -212.392600      0  \n",
      "2    0.436893    0.116505   0.126214        0.330097   -69.374078      0  \n",
      "3    0.420455    0.132576   0.159091        0.318182  -221.352273      0  \n",
      "4    0.303030    0.136364   0.189394        0.378788  -104.035909      1  \n",
      "âœ… Saved: /content/drive/MyDrive/ai_human_detection/linguistic_features.csv\n"
     ]
    }
   ],
   "source": [
    "# STEP: Linguistic Feature Extraction\n",
    "\n",
    "import pandas as pd\n",
    "import textstat\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "print(\"Dataset loaded:\", df.shape)\n",
    "\n",
    "def extract_linguistic_features(text):\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    wc = len(words)\n",
    "    sc = len(sentences)\n",
    "\n",
    "    lexical_div = len(set(words)) / wc if wc else 0\n",
    "\n",
    "    noun_ratio = sum(1 for _, t in pos_tags if t.startswith(\"NN\")) / wc if wc else 0\n",
    "    verb_ratio = sum(1 for _, t in pos_tags if t.startswith(\"VB\")) / wc if wc else 0\n",
    "    adj_ratio  = sum(1 for _, t in pos_tags if t.startswith(\"JJ\")) / wc if wc else 0\n",
    "    stop_ratio = sum(1 for w in words if w in stop_words) / wc if wc else 0\n",
    "\n",
    "    readability = textstat.flesch_reading_ease(text)\n",
    "\n",
    "    return [\n",
    "        wc,\n",
    "        sc,\n",
    "        wc / sc if sc else 0,\n",
    "        lexical_div,\n",
    "        noun_ratio,\n",
    "        verb_ratio,\n",
    "        adj_ratio,\n",
    "        stop_ratio,\n",
    "        readability\n",
    "    ]\n",
    "\n",
    "print(\"Extracting linguistic features...\")\n",
    "\n",
    "features = df[\"text\"].apply(extract_linguistic_features)\n",
    "\n",
    "ling_df = pd.DataFrame(\n",
    "    features.tolist(),\n",
    "    columns=[\n",
    "        \"word_count\",\n",
    "        \"sentence_count\",\n",
    "        \"avg_sentence_length\",\n",
    "        \"lexical_diversity\",\n",
    "        \"noun_ratio\",\n",
    "        \"verb_ratio\",\n",
    "        \"adj_ratio\",\n",
    "        \"stopword_ratio\",\n",
    "        \"readability\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "ling_df[\"label\"] = df[\"label\"]\n",
    "\n",
    "print(\"Linguistic features extracted successfully\")\n",
    "print(ling_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39303,
     "status": "ok",
     "timestamp": 1770451068252,
     "user": {
      "displayName": "4102_Ravi Chaudhary",
      "userId": "03292946989340673993"
     },
     "user_tz": -330
    },
    "id": "fdBc8BvV_O3f",
    "outputId": "c612d1c7-b64d-4b8f-fe28-f4af0a815eca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: (21612, 2)\n",
      "ðŸ”„ Fitting TF-IDF vectorizer...\n",
      "âœ… TF-IDF extraction completed\n",
      "TF-IDF shape: (21612, 5000)\n"
     ]
    }
   ],
   "source": [
    "# STEP: TF-IDF Feature Extraction\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"Dataset loaded:\", df.shape)\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.9,\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "print(\"Fitting TF-IDF vectorizer...\")\n",
    "X_tfidf = tfidf.fit_transform(df[\"text\"])\n",
    "\n",
    "print(\"TF-IDF extraction completed\")\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14030,
     "status": "ok",
     "timestamp": 1770451082285,
     "user": {
      "displayName": "4102_Ravi Chaudhary",
      "userId": "03292946989340673993"
     },
     "user_tz": -330
    },
    "id": "f2DjMjSr_3E4",
    "outputId": "f593ba9c-b9b1-497d-acbe-543e74f5c8f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text samples: (21612, 2)\n",
      "Linguistic features: (21612, 9)\n",
      "TF-IDF shape: (21612, 5000)\n",
      "Combined feature shape: (21612, 5009)\n",
      "âœ… Split complete\n",
      "Training samples: 17289\n",
      "Testing samples : 4323\n"
     ]
    }
   ],
   "source": [
    "# STEP: Feature Fusion + Train/Test Split\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Labels\n",
    "y = df[\"label\"].values\n",
    "\n",
    "# Linguistic features (drop label)\n",
    "ling_features = ling_df.drop(columns=[\"label\"]).values\n",
    "\n",
    "print(\"Text samples:\", df.shape)\n",
    "print(\"Linguistic features:\", ling_features.shape)\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)\n",
    "\n",
    "# Feature fusion\n",
    "X_combined = hstack([X_tfidf, ling_features])\n",
    "print(\"Combined feature shape:\", X_combined.shape)\n",
    "\n",
    "# Train/Test split (80/20, stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Split complete\")\n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Testing samples :\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 101776,
     "status": "ok",
     "timestamp": 1770451184079,
     "user": {
      "displayName": "4102_Ravi Chaudhary",
      "userId": "03292946989340673993"
     },
     "user_tz": -330
    },
    "id": "yFnkG5jfJp5r",
    "outputId": "5bec93c9-866d-4c0b-e245-31e6e84f9fa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest classifier...\n",
      "\n",
      " Evaluation Results\n",
      "Accuracy : 0.9783\n",
      "Macro F1 : 0.9783\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9868    0.9695    0.9781      2165\n",
      "           1     0.9699    0.9870    0.9784      2158\n",
      "\n",
      "    accuracy                         0.9783      4323\n",
      "   macro avg     0.9784    0.9783    0.9783      4323\n",
      "weighted avg     0.9784    0.9783    0.9783      4323\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[2099   66]\n",
      " [  28 2130]]\n",
      "\n",
      " Random Forest model saved\n"
     ]
    }
   ],
   "source": [
    "# STEP: Train & Evaluate Random Forest\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Convert sparse matrix to dense (required for Random Forest)\n",
    "X_train_dense = X_train.toarray()\n",
    "X_test_dense = X_test.toarray()\n",
    "\n",
    "# Initialize classifier\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"Training Random Forest classifier...\")\n",
    "rf_clf.fit(X_train_dense, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = rf_clf.predict(X_test_dense)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "report = classification_report(y_test, y_pred, digits=4)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"\\nEvaluation Results\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Macro F1 : {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1770453316312,
     "user": {
      "displayName": "4102_Ravi Chaudhary",
      "userId": "03292946989340673993"
     },
     "user_tz": -330
    },
    "id": "pXFWdnwSK6pS",
    "outputId": "33e8baee-c201-44fa-9313-e7644b52a5d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     prediction  \\\n",
      "0  This paper presents an experimental evaluation...   AI-Generated   \n",
      "1  As an AI language model, I generate responses ...  Human-Written   \n",
      "\n",
      "   prob_human  prob_ai  \n",
      "0       0.165    0.835  \n",
      "1       0.755    0.245  \n"
     ]
    }
   ],
   "source": [
    "# STEP: Test on New / Unseen Text\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textstat\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def extract_linguistic_features(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    wc = len(words)\n",
    "    sc = len(sentences)\n",
    "\n",
    "    lexical_div = len(set(words)) / wc if wc else 0\n",
    "    noun_ratio = sum(1 for _, t in pos_tags if t.startswith(\"NN\")) / wc if wc else 0\n",
    "    verb_ratio = sum(1 for _, t in pos_tags if t.startswith(\"VB\")) / wc if wc else 0\n",
    "    adj_ratio  = sum(1 for _, t in pos_tags if t.startswith(\"JJ\")) / wc if wc else 0\n",
    "    stop_ratio = sum(1 for w in words if w in stop_words) / wc if wc else 0\n",
    "    readability = textstat.flesch_reading_ease(text)\n",
    "\n",
    "    return [\n",
    "        wc, sc, wc / sc if sc else 0,\n",
    "        lexical_div, noun_ratio, verb_ratio,\n",
    "        adj_ratio, stop_ratio, readability\n",
    "    ]\n",
    "\n",
    "def predict_text(texts):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    X_tfidf_new = tfidf.transform(texts)\n",
    "    ling_features = np.array([extract_linguistic_features(t) for t in texts])\n",
    "\n",
    "    X_combined = hstack([X_tfidf_new, ling_features]).toarray()\n",
    "\n",
    "    preds = rf_clf.predict(X_combined)\n",
    "    probs = rf_clf.predict_proba(X_combined)\n",
    "\n",
    "    results = []\n",
    "    for text, p, prob in zip(texts, preds, probs):\n",
    "        label = \"Human-Written\" if p == 0 else \"AI-Generated\"\n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"prediction\": label,\n",
    "            \"prob_human\": round(prob[0], 4),\n",
    "            \"prob_ai\": round(prob[1], 4)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example\n",
    "samples = [\n",
    "    \"This paper presents an experimental evaluation conducted by the authors.\",\n",
    "    \"As an AI language model, I generate responses based on training data.\"\n",
    "]\n",
    "\n",
    "print(predict_text(samples))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPd6+xjxSBsyLpn2WcHe5hW",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
